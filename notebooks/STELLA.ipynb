{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36496ea5-fb7f-4f92-9938-d6cbe8ba9fc6",
   "metadata": {},
   "source": [
    "# STELLA Transcription Dataset\n",
    "\n",
    "The STELLA dataset...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de1803-c9e1-4b84-9e30-0e8e3a7dea2d",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "Format Dataset into the wanted architecture. This procedure extracts audiobook transcriptions from the original dataset and sorts them into the same splits as the audio files.\n",
    "\n",
    "```\n",
    "txt\n",
    "├── LANG\n",
    "│   ├── HOUR_SPLIT\n",
    "│   │   ├── SECTION_SPLIT\n",
    "│   │   │   ├── books.txt\n",
    "│   │   │   ├── meta.json\n",
    "│   │   │   └── transcription.txt\n",
    "│   │   ├── ...\n",
    "│   ├── ...\n",
    "│   ...\n",
    "\n",
    "```\n",
    "- txt : folder containing transcriptions\n",
    "- LANG: corresponds to the given language\n",
    "- HOUR_SPLIR: corresponds to the size of the section splits in number of hours of speech,\n",
    "              formatted as (50h, 100h, ..., 3200h)\n",
    "- SECTION_SPLIT: separation of content into sections with equal amount of speech content.\n",
    "- books.txt: the list of books used for this split\n",
    "- meta.json: metadata generated during clean-up used to measure effectiveness of cleaning.\n",
    "- transcript.txt: the agregated transcripts of the audiobooks in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b9fc4b-3337-47e2-9808-e3c5ac0ba45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Succesfuly build STELA Transcript dataset !</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mSuccesfuly build STELA Transcript dataset !\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lexical_benchmark.datasets.machine import stella\n",
    "from rich.console import Console\n",
    "\n",
    "out = Console()\n",
    "\n",
    "prep = stella.STELAPrepTranscripts(lang=\"EN\")\n",
    "with out.status(\"Processing STELA source...\"):\n",
    "    prep.build_transcript()\n",
    "\n",
    "out.print(\"Succesfuly build STELA Transcript dataset !\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267202f-41b0-44b8-860e-0014325b6bfa",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Clean-up text to keep only clean words that can be piped through the dictionairy.\n",
    "\n",
    "RULES (Order Matters):\n",
    "1) Illustration tag removal\n",
    "2) URL removal\n",
    "3) TextNormalisation : correct accents & remove non-printable characters\n",
    "4) Trancribe numbers\n",
    "5) Remove roman numerals\n",
    "6) Fix symbols ($,€, etc..)\n",
    "7) AZFilter\n",
    "\n",
    "    * replace '-' with a space to extract hyphenated words (fifty-five -> fifty five)\n",
    "\n",
    "    * Keeps apostrophe char(*'*) to protect shorthands (ex: ain't)\n",
    "  \n",
    "    * purges everything not between [A-Z].\n",
    "\n",
    "    * lowecases everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebc865a-0bd3-45a4-b237-bd12db002d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21de4c6ed70e478baaf4e5eb0b54774a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Succesfuly cleanned up STELA Transcript dataset !</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mSuccesfuly cleanned up STELA Transcript dataset !\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lexical_benchmark.datasets.machine import stella\n",
    "from rich.console import Console\n",
    "\n",
    "out = Console()\n",
    "\n",
    "cleaner = stella.StelaCleaner()\n",
    "cleaner.mk_clean(show_progress=True)\n",
    "out.print(\"Succesfuly cleanned up STELA Transcript dataset !\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a850a9-ef40-422b-b7b6-0967efc8de16",
   "metadata": {},
   "source": [
    "# Word Filtering\n",
    "\n",
    "Using a pre-selected lexicon we filter the corpus to separated known from unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d5c1f6-dc95-4640-bd33-e5a9ce71191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:: /scratch1/projects/lexical-benchmark/v2/datasets/clean/StelaTrainDataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed cleaning of words!!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed cleaning of words!!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lexical_benchmark.datasets.machine import stella\n",
    "from lexical_benchmark.datasets import utils\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "dataset = stella.STELLADatasetClean()\n",
    "# BUG: for now only en exists we need different loopping if more languages are added \n",
    "en_cleaner = utils.DictionairyCleaner(lang=\"EN\")\n",
    "print(f'Working on:: {dataset.root_dir}')\n",
    "\n",
    "with console.status(\"Computing clean words\"):\n",
    "    for _, (lang, hour_split, section) in enumerate(dataset.iter_all()):\n",
    "        # Create clean, unclean subsets of transcriptions\n",
    "        dataset.filter_words(lang, hour_split, section, cleaner=en_cleaner)\n",
    "    \n",
    "        # Compute word frequencies of both items\n",
    "        _ = dataset.clean_words_freq(lang, hour_split, section)\n",
    "        _ = dataset.rejected_words_freq(lang, hour_split, section)\n",
    "console.print(\"Completed cleaning of words!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53ce55-03d1-4838-9cce-c4dca3792d2e",
   "metadata": {},
   "source": [
    "## Computing cleanup stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b315e7-490e-4e37-b269-f8fe7417022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexical_benchmark.datasets.machine import stella\n",
    "from lexical_benchmark.datasets import utils\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "dataset = stella.STELLADatasetClean()\n",
    "all_stats = {}\n",
    "for _, (lang, hour_split, section) in enumerate(dataset.iter_all()):\n",
    "    all_stats[(lang, hour_split, section)] = dataset.word_cleaning_stats(lang, hour_split, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9095026d-9dc4-4fce-b91b-3915e4dcee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "      <th>bad_percent</th>\n",
       "      <th>good_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EN_100h_00</th>\n",
       "      <td>32092</td>\n",
       "      <td>27967</td>\n",
       "      <td>4102</td>\n",
       "      <td>12.782002</td>\n",
       "      <td>87.146329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_100h_01</th>\n",
       "      <td>39002</td>\n",
       "      <td>31161</td>\n",
       "      <td>7778</td>\n",
       "      <td>19.942567</td>\n",
       "      <td>79.895903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_100h_02</th>\n",
       "      <td>34277</td>\n",
       "      <td>28818</td>\n",
       "      <td>5381</td>\n",
       "      <td>15.698573</td>\n",
       "      <td>84.073869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_100h_03</th>\n",
       "      <td>31719</td>\n",
       "      <td>29064</td>\n",
       "      <td>2602</td>\n",
       "      <td>8.203285</td>\n",
       "      <td>91.629623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_100h_04</th>\n",
       "      <td>33701</td>\n",
       "      <td>29953</td>\n",
       "      <td>3730</td>\n",
       "      <td>11.067921</td>\n",
       "      <td>88.878668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_50h_63</th>\n",
       "      <td>21496</td>\n",
       "      <td>20320</td>\n",
       "      <td>1176</td>\n",
       "      <td>5.470785</td>\n",
       "      <td>94.529215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_800h_00</th>\n",
       "      <td>115890</td>\n",
       "      <td>73803</td>\n",
       "      <td>42087</td>\n",
       "      <td>36.316334</td>\n",
       "      <td>63.683666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_800h_01</th>\n",
       "      <td>211138</td>\n",
       "      <td>86743</td>\n",
       "      <td>124395</td>\n",
       "      <td>58.916443</td>\n",
       "      <td>41.083557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_800h_02</th>\n",
       "      <td>149307</td>\n",
       "      <td>81359</td>\n",
       "      <td>67948</td>\n",
       "      <td>45.508918</td>\n",
       "      <td>54.491082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN_800h_03</th>\n",
       "      <td>123836</td>\n",
       "      <td>75284</td>\n",
       "      <td>48552</td>\n",
       "      <td>39.206693</td>\n",
       "      <td>60.793307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               all   good     bad  bad_percent  good_percent\n",
       "EN_100h_00   32092  27967    4102    12.782002     87.146329\n",
       "EN_100h_01   39002  31161    7778    19.942567     79.895903\n",
       "EN_100h_02   34277  28818    5381    15.698573     84.073869\n",
       "EN_100h_03   31719  29064    2602     8.203285     91.629623\n",
       "EN_100h_04   33701  29953    3730    11.067921     88.878668\n",
       "...            ...    ...     ...          ...           ...\n",
       "EN_50h_63    21496  20320    1176     5.470785     94.529215\n",
       "EN_800h_00  115890  73803   42087    36.316334     63.683666\n",
       "EN_800h_01  211138  86743  124395    58.916443     41.083557\n",
       "EN_800h_02  149307  81359   67948    45.508918     54.491082\n",
       "EN_800h_03  123836  75284   48552    39.206693     60.793307\n",
       "\n",
       "[127 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def m(lang, hour_split, section):\n",
    "    return f\"{lang}_{hour_split}_{section}\"\n",
    "\n",
    "data = {f\"{m(*k)}\":v for k, v in all_stats.items()}\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "909529d6-4077-4e30-b705-83a47a1e6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a 31.6198% of word elimination !!\n",
      "We have a 68.3714% of word retention !!\n",
      "Average removal per set is   21.12%\n",
      "Average retention per set is 78.87%\n"
     ]
    }
   ],
   "source": [
    "all_count = df[\"all\"].sum()\n",
    "bad_count = df[\"bad\"].sum()\n",
    "good_count = df[\"good\"].sum()\n",
    "percent_bad = (bad_count / all_count)\n",
    "percent_good = (good_count / all_count)\n",
    "\n",
    "print(f\"We have a {percent_bad:.4%} of word elimination !!\")\n",
    "print(f\"We have a {percent_good:.4%} of word retention !!\")\n",
    "print(f\"Average removal per set is   {df['bad_percent'].mean():.4}%\")\n",
    "print(f\"Average retention per set is {df['good_percent'].mean():.4}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
