{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b33cc8-b09a-4bc3-84d7-431af3ca9679",
   "metadata": {},
   "source": [
    "# Wordlist filtering\n",
    "\n",
    "We filter words to separate valid from invalid speech, we do it using specific lexicon dictionairies.\n",
    "\n",
    "All the dictionnairies aggragated give us an approximate of 1,534,810 words. Which gives us a good coverage of the english language.\n",
    "The ideal would be to use the Google Book's corpus with more that 2M words but it is not open-source (I think?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a076d76a-0996-41ae-a83b-0e338a27fe0a",
   "metadata": {},
   "source": [
    "### SCOWLv2 - ~676k words\n",
    "\n",
    "[Aspell](http://wordlist.aspell.net) is a spellchecker created by open-source communities, and\n",
    "variations of it is used by Mozzila, OpenOffice and various linux distributions.\n",
    "\n",
    "Their site has various dictionairies & tools to make spellcheckers.\n",
    "\n",
    "We use their tool in this page to generate a wordlist => [Create wordlist](http://app.aspell.net/create)\n",
    "\n",
    "The following parameters are used :\n",
    "\n",
    "- diacritic=strip (replaces words like café with cafe)\n",
    "- max_size=95 (this is the biggest available has **~676k words**)\n",
    "- max_variant=3 (this is the biggest available allows multiple variants of the same word)\n",
    "- special=hacker (includes computer science related vocabulairy, we excluded roman numerals)\n",
    "- spelling: US, GB( s & z variants)\n",
    "\n",
    "\n",
    "**CITATION**: None copyright notice in github gives various thanks to open-sourced projects that contributed to lexicon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79796f9f-44f4-4811-823a-f32143290995",
   "metadata": {},
   "source": [
    "### Kaiki.org - ~1M words\n",
    "\n",
    "Kaikki.org is a digital archive and a data mining group. We aim to make our digital heritage more \n",
    "accessible and useful for people, researchers, linguists, software developers, and artificial \n",
    "intelligence (AI).\n",
    "\n",
    "We used the All-English forms which is their biggest dictionairy for english available you can find it [here](https://kaikki.org/dictionary/English/index.html).\n",
    "\n",
    "We did some post-processing to extract only the words from the dictionairy.\n",
    "\n",
    "\n",
    "**CITATION**:  If you use this data in academic research, please cite [Tatu Ylonen: Wiktextract: Wiktionary as Machine-Readable Structured Data](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.140.pdf), Proceedings of the 13th Conference on Language Resources and Evaluation (LREC), pp. 1317-1325, Marseille, 20-25 June 2022. Linking to the relevant page(s) under https://kaikki.org would also be greatly appreciated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f049547-a652-45fe-8a0e-72974d506da8",
   "metadata": {},
   "source": [
    "### Yet Another Word List (YAWL) - ~264k words\n",
    "\n",
    "Open source word-list found here : https://github.com/elasticdog/yawl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5529d-3d9a-44eb-86a7-652a72b0b403",
   "metadata": {},
   "source": [
    "### CHILDES extras\n",
    "\n",
    "Using the tags from CHILDES we create a lexicon of non-words words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601c2f2-a257-4cc2-9f15-842ed7cb8fe9",
   "metadata": {},
   "source": [
    "## Data Organisation\n",
    "\n",
    "Each subset has been processed to a lexicon folder with the following format :\n",
    "\n",
    "```\n",
    "lexicon/\n",
    "├── kaikki\n",
    "│   ├── english-dictionairy.jsonl\n",
    "│   ├── README.md\n",
    "│   └── words.list\n",
    "├── SCOWLv2\n",
    "│   ├── README\n",
    "│   ├── README_SCOWL\n",
    "│   └── words.list\n",
    "└── yawl\n",
    "    ├── README.md\n",
    "    └── words.list\n",
    "```\n",
    "\n",
    "The word-lists are formatted into text files containing one entry per line named `words.list`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac010c-e362-4022-8903-918f50a3c6b0",
   "metadata": {},
   "source": [
    "# Kaiki.org post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3834614f-702a-4e1f-80af-6c4fed02a43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">len(words)=1226594\n",
       "</pre>\n"
      ],
      "text/plain": [
       "len(words)=1226594\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "dict_file = Path(\"/scratch1/projects/lexical-benchmark/v2/datasets/lexicon/kaikki/english-dictionairy.jsonl\")\n",
    "words_file = Path(\"/scratch1/projects/lexical-benchmark/v2/datasets/lexicon/kaikki/wordlist.txt\")\n",
    "def get_word(line: bytes) -> str | None:\n",
    "    \"\"\"Extract word.\"\"\"\n",
    "    data = json.loads(line)\n",
    "    if \"word\" in data:\n",
    "        return data[\"word\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "with dict_file.open() as f_dict, words_file.open(\"w\") as f_words, console.status(\"Extracting kaiki.org english word list...\"):\n",
    "    words = [get_word(item) for item in f_dict]\n",
    "    # Filter empty entries and only keep unique items\n",
    "    words = set([w for w in words if w is not None])\n",
    "    print(f\"{len(words)=}\")\n",
    "    # Dump into file\n",
    "    for w in words:\n",
    "        f_words.write(f\"{w}\\n\")\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f3ef5-0629-49a1-9a93-f47622813b28",
   "metadata": {},
   "source": [
    "## Testing result lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93f0b38d-e187-4644-a70e-0c601d83ac44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(kaikki)=1_226_594 words\n",
      "len(scowl)=675_611 words\n",
      "len(yawl)=264_097 words\n",
      "-----\n",
      "len(all_words)=1_534_810 words\n",
      "-----\n",
      "len(kaikki_scowl)=1_534_524 words\n",
      "-----\n",
      "len(kaikki_yawl)=1_251_196 words\n",
      "-----\n",
      "len(scowl_yawl)=676_076 words\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "lexicon_root = Path(\"/scratch1/projects/lexical-benchmark/v2/datasets/lexicon/\")\n",
    "kaiki_file = lexicon_root / \"kaikki\" / \"words.list\"\n",
    "scowl_file = lexicon_root / \"SCOWLv2\" / \"words.list\"\n",
    "yawl_file = lexicon_root / \"yawl\" / \"words.list\"\n",
    "\n",
    "kaikki = set(kaiki_file.read_text().splitlines())\n",
    "scowl = set(scowl_file.read_text().splitlines())\n",
    "yawl = set(yawl_file.read_text().splitlines())\n",
    "\n",
    "print(f\"{len(kaikki)=:_} words\")\n",
    "print(f\"{len(scowl)=:_} words\")\n",
    "print(f\"{len(yawl)=:_} words\")\n",
    "\n",
    "print('-'*5)\n",
    "all_words = set()\n",
    "all_words.update(kaikki, scowl, yawl)\n",
    "print(f\"{len(all_words)=:_} words\")\n",
    "print('-'*5)\n",
    "kaikki_scowl = set()\n",
    "kaikki_scowl.update(kaiki, scowl)\n",
    "print(f\"{len(kaikki_scowl)=:_} words\")\n",
    "print('-'*5)\n",
    "kaikki_yawl = set()\n",
    "kaikki_yawl.update(kaikki, yawl)\n",
    "print(f\"{len(kaikki_yawl)=:_} words\")\n",
    "print('-'*5)\n",
    "scowl_yawl = set()\n",
    "scowl_yawl.update(scowl, yawl)\n",
    "print(f\"{len(scowl_yawl)=:_} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544fb6c-e7e6-4a98-be3e-79f69f67beb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
